---
title: |
    | Studying Rational Agency and Epistemic Communities with Large Language Models:
    | Review, How-To, and Reflection
subtitle: |
    | Workshop on Computational Models in Social Epistemology
    | Bochum, Dec 6-8 2023
author: "Gregor Betz (DebateLab@KIT)"
bibliography: references.bib
format:
  revealjs:
    theme: [default, custom.scss]
embed-resources: true
---

# Review

## Scratchpad

* review paper agents
* jasss paper
* tenenbaum debating paper 

# How-To

## Skeleton Bounded Confidence Agent

```{.python code-line-numbers="|3-5|7-12|14,17"}
class AbstractBCAgent():

    def update(self, community):
        opinions = [peer.opinion for peer in self.peers(community)]
        self.opinion = self.revise(opinions)

    def peers(self, community):
        peers = [
            agent for agent in community
            if self.distance(agent.opinion) <= epsilon
        ]
        return peers

    def distance(self, opinion):
        pass 

    def revise(self, opinions):
        pass
```

## Numerical BC Agent

```{.python code-line-numbers="|3-5|7-11"}
class NumericalBCAgent(AbstractBCAgent):

    def distance(self, opinion):
        """calculates distance between agent's and other opinion"""
        return abs(opinion - self.opinion)

    def revise(self, opinions):
        """revision through weighted opinion averaging"""
        alpha = self._parameters.get("alpha", .5)
        revision = alpha * self.opinion + (1-alpha) * np.mean(opinions)
        return revision
```

## Numerical BC Model: Results

::: {.r-stack .r-stretch}
![](figs/numbc_opevol.png){.fragment}

![](figs/numbc_diffevol.png){.fragment}
:::


## Large Language Model

```{.python code-line-numbers="2"}
model = lmql.model(
    "local:HuggingFaceH4/zephyr-7b-alpha",
    device_map = "auto",
    load_in_8bit=True,
    low_cpu_mem_usage=True
)
```

## Agreement Prompt

::: {.r-stack}
![](figs/lmql_agreement.png){width="900"}
:::

## Revision Prompt

::: {.r-stack}
![](figs/lmql_revision.png){width="900"}
:::

## Natural Language BC Agent


```{.python code-line-numbers="|5-7,13-15|8,9"}
class NaturalLanguageBCAgent(AbstractBCAgent):

    def distance(self, other):
        """distance as expected agreement level"""
        lmql_result = agreement_lmq(
            self.opinion, other, **kwargs
        )
        probs = lmql_result.variables.get("P(LABEL)")
        return sum([i*v for i, (_, v) in enumerate(probs)])/4.0

    def revise(self, peer_opinions):
        """natural language opinion revision"""
        revision = revise_lmq(
            self.opinion, peer_opinions, **kwargs
        )
        return revision
```

## Natural Language BC Model: Results

`alpha`="very high"; `epsilon`=[**0.4**]{.fragment .strike fragment-index=2}[**0.5**]{.fragment .fade-in fragment-index=2}; `topic`="veganism"


::: {.r-stack .r-stretch}
![](figs/nlbc_diffevol40.png){.fragment fragment-index=1}

![](figs/nlbc_diffevol50.png){.fragment fragment-index=2}
:::

## Scratchpad

```{.python code-line-numbers="5-6"}
import numpy as np

class AbstractClass():

  def method(self, a):
    pass
```


# Reflection

## Scratchpad

Why do this?

* Re-create and probe our epistemological (computational) models.
* Simulate and test scientific methodologies, reasoning modes, principles of rationality. Any!  (E.g. value-free ideal.)

LLMs apt for these purposes?

Papers:

* self-correction (-> reflexive AI)
* levels of AGI [@morris2023levels]
* AI scientists [@ai4science2023impact]
* LLMs as rational doxastic and epistemic agents (DoxLM)
* (But humans' cognitive architecture is fundamtally different from LLMs', or is it?)


* The neural architecture of language: Integrative modeling converges on predictive processing
Martin Schrimpf et al. 2021
  * TLDR: It is found that the most powerful “transformer” models predict nearly 100% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography).
* Brains and algorithms partially converge in natural language processing
C. CaucheteuxJ. King 2022
  * TLDR This study shows that modern language algorithms partially converge towards brain-like solutions, and thus delineates a promising path to unravel the foundations of natural language processing.
* Mapping Brains with Language Models: A Survey Antonia KaramolegkouMostafa AbdouAnders Søgaard Computer Science, Linguistics Annual Meeting of the Association for… 2023
  * ABSTRACT [...] We also find that the accumulated evidence, for now, remains ambiguous, but correlations with model size and quality provide grounds for cautious optimism.
* Artificial neural network language models predict human brain responses to language even after a developmentally realistic amount of training. Eghbal A. HosseiniMartin SchrimpfYian ZhangSamuel R. BowmanNoga ZaslavskyEvelina Fedorenko Computer Science, Psychology 2023
  * TLDR [A] developmentally realistic amount of training may suffice and [...] models that have received enough training to achieve sufficiently high next-word prediction performance also acquire representations of sentences that are predictive of human fMRI responses.




Conclusion:

* LLMs apt for these purposes? ✅
* vanishing distinctions:
  - simulating science vs doing science
  - epistemology vs AI
* AGI => epistemic redundancy:
  - what role for humans in the science system
  - what is AI-proof well-ordered science (science in a democracy?)
  - science's ultimate "Freudian" offence and blow to humans' collective narcissism?



# Backup

## References

::: {#refs}
:::