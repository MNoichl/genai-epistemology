@misc{ai4science2023impact,
      title={The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4}, 
      author={Microsoft Research AI4Science and Microsoft Azure Quantum},
      year={2023},
      eprint={2311.07361},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{morris2023levels,
      title={Levels of AGI: Operationalizing Progress on the Path to AGI}, 
      author={Meredith Ringel Morris and Jascha Sohl-dickstein and Noah Fiedel and Tris Warkentin and Allan Dafoe and Aleksandra Faust and Clement Farabet and Shane Legg},
      year={2023},
      eprint={2311.02462},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{Goldstein2020ThinkingAS,
  title={Thinking ahead: spontaneous prediction in context as a keystone of language in humans and machines},
  author={Ariel Goldstein and Zaid Zada and Eliav Buchnik and Mariano Schain and Amy Rose Price and Bobbi Aubrey and Samuel A. Nastase and Amir Feder and Dotan Emanuel and Alon Cohen and Aren Jansen and Harshvardhan Gazula and Gina Choe and Aditi Rao and Catherine Kim and Colton Casto and Fanda Lora and Adeen Flinker and Sasha Devore and Werner K. Doyle and Daniel Friedman and Patricia Dugan and Avinatan Hassidim and Michael P. Brenner and Yossi Matias and Kenneth A. Norman and Orrin Devinsky and Uri Hasson},
  journal={bioRxiv},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:227915127}
}
@misc{pan2023automatically,
      title={Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies}, 
      author={Liangming Pan and Michael Saxon and Wenda Xu and Deepak Nathani and Xinyi Wang and William Yang Wang},
      year={2023},
      eprint={2308.03188},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{du2023improving,
      title={Improving Factuality and Reasoning in Language Models through Multiagent Debate}, 
      author={Yilun Du and Shuang Li and Antonio Torralba and Joshua B. Tenenbaum and Igor Mordatch},
      year={2023},
      eprint={2305.14325},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wang2023survey,
      title={A Survey on Large Language Model based Autonomous Agents}, 
      author={Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and Wayne Xin Zhao and Zhewei Wei and Ji-Rong Wen},
      year={2023},
      eprint={2308.11432},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{betz2022,
	abstract = {This paper develops a natural-language agent-based model of argumentation (ABMA). Its artificial deliberative agents (ADAs) are constructed with the help of so-called neural language models recently developed in AI and computational linguistics. ADAs are equipped with a minimalist belief system and may generate and submit novel contributions to a conversation. The natural-language ABMA allows us to simulate collective deliberation in English, i.e. with arguments, reasons, and claims themselves &mdash; rather than with their mathematical representations (as in symbolic models). This paper uses the natural-language ABMA to test the robustness of symbolic reason-balancing models of argumentation (M\"{a}s & Flache 2013; Singer et al. 2019): First of all, as long as ADAs remain passive, confirmation bias and homophily updating trigger polarization, which is consistent with results from symbolic models. However, once ADAs start to actively generate new contributions, the evolution of a conversation is dominated by properties of the agents as authors. This suggests that the creation of new arguments, reasons, and claims critically affects a conversation and is of pivotal importance for understanding the dynamics of collective deliberation. The paper closes by pointing out further fruitful applications of the model and challenges for future research.},
	author = {Betz, Gregor},
	doi = {10.18564/jasss.4725},
	issn = {1460-7425},
	journal = {Journal of Artificial Societies and Social Simulation},
	keywords = {Opinion Dynamics, Argumentation, Natural Language Processing, Language Model},
	number = {1},
	pages = {2},
	title = {Natural-Language Multi-Agent Simulations of Argumentative Opinion Dynamics},
	url = {http://jasss.soc.surrey.ac.uk/25/1/2.html},
	volume = {25},
	year = {2022},
}
@article{BetzRichardson2023,
    doi = {10.1371/journal.pone.0281372},
    author = {Gregor Betz and Kyle Richardson},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Probabilistic coherence, logical consistency, and Bayesian learning: Neural language models as epistemic agents},
    year = {2023},
    month = {02},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pone.0281372},
    pages = {1-29},
    abstract = {It is argued that suitably trained neural language models exhibit key properties of epistemic agency: they hold probabilistically coherent and logically consistent degrees of belief, which they can rationally revise in the face of novel evidence. To this purpose, we conduct computational experiments with rankers: T5 models [Raffel et al. 2020] that are pretrained on carefully designed synthetic corpora. Moreover, we introduce a procedure for eliciting a model’s degrees of belief, and define numerical metrics that measure the extent to which given degrees of belief violate (probabilistic, logical, and Bayesian) rationality constraints. While pretrained rankers are found to suffer from global inconsistency (in agreement with, e.g., [Jang et al. 2021]), we observe that subsequent self-training on auto-generated texts allows rankers to gradually obtain a probabilistically coherent belief system that is aligned with logical constraints. In addition, such self-training is found to have a pivotal role in rational evidential learning, too, for it seems to enable rankers to propagate a novel evidence item through their belief systems, successively re-adjusting individual degrees of belief. All this, we conclude, confirms the Rationality Hypothesis, i.e., the claim that suitable trained NLMs may exhibit advanced rational skills. We suggest that this hypothesis has empirical, yet also normative and conceptual ramifications far beyond the practical linguistic problems NLMs have originally been designed to solve.},
    number = {2},
}
@article{
doi:10.1073/pnas.2105646118,
author = {Martin Schrimpf  and Idan Asher Blank  and Greta Tuckute  and Carina Kauf  and Eghbal A. Hosseini  and Nancy Kanwisher  and Joshua B. Tenenbaum  and Evelina Fedorenko },
title = {The neural architecture of language: Integrative modeling converges on predictive processing},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {45},
pages = {e2105646118},
year = {2021},
doi = {10.1073/pnas.2105646118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2105646118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2105646118},
abstract = {The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species’ signature cognitive skill. We find that the most powerful “transformer” models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models’ neural fits (“brain score”) and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.}}
@article{caucheteux2022brains,
  title={Brains and algorithms partially converge in natural language processing},
  author={Caucheteux, Charlotte and King, Jean-R{\'e}mi},
  journal={Communications biology},
  volume={5},
  number={1},
  pages={134},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@article{karamolegkou2023mapping,
  title={Mapping Brains with Language Models: A Survey},
  author={Karamolegkou, Antonia and Abdou, Mostafa and S{\o}gaard, Anders},
  journal={arXiv preprint arXiv:2306.05126},
  year={2023}
}
@article{hosseini2022artificial,
  title={Artificial neural network language models align neurally and behaviorally with humans even after a developmentally realistic amount of training},
  author={Hosseini, Eghbal A and Schrimpf, Martin and Zhang, Yian and Bowman, Samuel and Zaslavsky, Noga and Fedorenko, Evelina},
  journal={BioRxiv},
  pages={2022--10},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}
